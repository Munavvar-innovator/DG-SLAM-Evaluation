"""
DG-SLAM (Hybrid pose estimation Stage 1 - using DRIOD SLAM )

EECE 5550 Mobile Robotics Project
"""

# ============================================================================
# SETUP
# ============================================================================

import subprocess
import sys
import os

print("Installing dependencies...")
deps = ["gdown", "opencv-python", "matplotlib", "tqdm", "scipy"]
for dep in deps:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", dep],
                         stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import cv2
import matplotlib.pyplot as plt
from pathlib import Path
from tqdm import tqdm
from PIL import Image

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Device: {device}")

# ============================================================================
# DOWNLOAD PRETRAINED WEIGHTS
# ============================================================================

def download_pretrained_weights():
    checkpoint_dir = Path('./checkpoints')
    checkpoint_dir.mkdir(exist_ok=True)
    checkpoint_file = checkpoint_dir / 'droid.pth'
    
    if checkpoint_file.exists() and checkpoint_file.stat().st_size > 1000000:
        print(f"Found checkpoint: {checkpoint_file}")
        return str(checkpoint_file)
    
    print("Downloading DROID pretrained weights...")
    import gdown
    file_id = "1PpqVt1H4maBa_GbPJp4NwxRsd9jk-elh"
    url = f"https://drive.google.com/uc?id={file_id}"
    gdown.download(url, str(checkpoint_file), quiet=False)
    print("Download complete!")
    return str(checkpoint_file)

CHECKPOINT_PATH = download_pretrained_weights()

# ============================================================================
# SE(3) UTILITIES
# ============================================================================

def quaternion_to_matrix(quat_pose):
    """[N,7] (qw,qx,qy,qz,tx,ty,tz) -> [N,4,4]"""
    single = quat_pose.dim() == 1
    if single:
        quat_pose = quat_pose.unsqueeze(0)
    
    N = quat_pose.shape[0]
    device = quat_pose.device
    
    q = quat_pose[:, :4] / (torch.norm(quat_pose[:, :4], dim=1, keepdim=True) + 1e-8)
    t = quat_pose[:, 4:]
    
    w, x, y, z = q[:, 0], q[:, 1], q[:, 2], q[:, 3]
    
    R = torch.zeros(N, 3, 3, device=device)
    R[:, 0, 0] = 1 - 2*(y*y + z*z)
    R[:, 0, 1] = 2*(x*y - w*z)
    R[:, 0, 2] = 2*(x*z + w*y)
    R[:, 1, 0] = 2*(x*y + w*z)
    R[:, 1, 1] = 1 - 2*(x*x + z*z)
    R[:, 1, 2] = 2*(y*z - w*x)
    R[:, 2, 0] = 2*(x*z - w*y)
    R[:, 2, 1] = 2*(y*z + w*x)
    R[:, 2, 2] = 1 - 2*(x*x + y*y)
    
    T = torch.eye(4, device=device).unsqueeze(0).repeat(N, 1, 1)
    T[:, :3, :3] = R
    T[:, :3, 3] = t
    
    return T[0] if single else T

def matrix_to_quaternion(T):
    """[N,4,4] -> [N,7]"""
    single = T.dim() == 2
    if single:
        T = T.unsqueeze(0)
    
    N = T.shape[0]
    device = T.device
    quat_pose = torch.zeros(N, 7, device=device)
    
    for i in range(N):
        R = T[i, :3, :3]
        t = T[i, :3, 3]
        trace = R[0, 0] + R[1, 1] + R[2, 2]
        
        if trace > 0:
            s = 0.5 / torch.sqrt(trace + 1.0)
            w = 0.25 / s
            x = (R[2, 1] - R[1, 2]) * s
            y = (R[0, 2] - R[2, 0]) * s
            z = (R[1, 0] - R[0, 1]) * s
        else:
            if R[0, 0] > R[1, 1] and R[0, 0] > R[2, 2]:
                s = 2.0 * torch.sqrt(1.0 + R[0, 0] - R[1, 1] - R[2, 2])
                w = (R[2, 1] - R[1, 2]) / s
                x = 0.25 * s
                y = (R[0, 1] + R[1, 0]) / s
                z = (R[0, 2] + R[2, 0]) / s
            elif R[1, 1] > R[2, 2]:
                s = 2.0 * torch.sqrt(1.0 + R[1, 1] - R[0, 0] - R[2, 2])
                w = (R[0, 2] - R[2, 0]) / s
                x = (R[0, 1] + R[1, 0]) / s
                y = 0.25 * s
                z = (R[1, 2] + R[2, 1]) / s
            else:
                s = 2.0 * torch.sqrt(1.0 + R[2, 2] - R[0, 0] - R[1, 1])
                w = (R[1, 0] - R[0, 1]) / s
                x = (R[0, 2] + R[2, 0]) / s
                y = (R[1, 2] + R[2, 1]) / s
                z = 0.25 * s
        
        quat_pose[i, :4] = torch.tensor([w, x, y, z], device=device)
        quat_pose[i, 4:] = t
    
    return quat_pose[0] if single else quat_pose

def exponential_map(xi):
    """se(3) -> SE(3)"""
    single = xi.dim() == 1
    if single:
        xi = xi.unsqueeze(0)
    
    N = xi.shape[0]
    device = xi.device
    omega = xi[:, :3]
    v = xi[:, 3:]
    T = torch.zeros(N, 4, 4, device=device)
    
    for i in range(N):
        theta = torch.norm(omega[i])
        
        if theta < 1e-6:
            T[i] = torch.eye(4, device=device)
            T[i, :3, 3] = v[i]
        else:
            w = omega[i] / theta
            K = torch.tensor([
                [0, -w[2], w[1]],
                [w[2], 0, -w[0]],
                [-w[1], w[0], 0]
            ], device=device)
            
            K2 = K @ K
            R = torch.eye(3, device=device) + torch.sin(theta)*K + (1-torch.cos(theta))*K2
            V = torch.eye(3, device=device) + ((1-torch.cos(theta))/theta)*K + \
                ((theta-torch.sin(theta))/theta)*K2
            
            T[i, :3, :3] = R
            T[i, :3, 3] = V @ v[i]
            T[i, 3, 3] = 1
    
    return T[0] if single else T

# ============================================================================
# DROID NETWORK
# ============================================================================

class DROIDNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        
        self.image_encoder = nn.Sequential(
            nn.Conv2d(3, 64, 7, stride=2, padding=3),
            nn.GroupNorm(8, 64),
            nn.ReLU(),
            nn.Conv2d(64, 128, 3, stride=2, padding=1),
            nn.GroupNorm(8, 128),
            nn.ReLU(),
            nn.Conv2d(128, 128, 3, padding=1),
            nn.GroupNorm(8, 128),
            nn.ReLU(),
        )
        
        self.depth_encoder = nn.Sequential(
            nn.Conv2d(1, 64, 7, stride=2, padding=3),
            nn.GroupNorm(8, 64),
            nn.ReLU(),
            nn.Conv2d(64, 128, 3, stride=2, padding=1),
            nn.GroupNorm(8, 128),
            nn.ReLU(),
        )
        
        self.gru = nn.GRUCell(256, 128)
        self.pose_delta = nn.Sequential(
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 6)
        )
        self.confidence = nn.Sequential(
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )
        
        self.pool = nn.AdaptiveAvgPool2d(1)
    
    def forward(self, images, depths, num_iterations=8):
        N = images.shape[0]
        device = images.device
        
        img_features = self.image_encoder(images)
        depth_features = self.depth_encoder(depths.unsqueeze(1))
        
        img_vec = self.pool(img_features).squeeze(-1).squeeze(-1)
        depth_vec = self.pool(depth_features).squeeze(-1).squeeze(-1)
        
        features = torch.cat([img_vec, depth_vec], dim=1)
        
        poses = torch.zeros(N, 7, device=device)
        poses[:, 0] = 1.0
        hidden = torch.zeros(N, 128, device=device)
        
        for _ in range(num_iterations):
            hidden = self.gru(features, hidden)
            delta = self.pose_delta(hidden)
            weight = self.confidence(hidden)
            
            for i in range(1, N):
                T_curr = quaternion_to_matrix(poses[i])
                T_delta = exponential_map(delta[i] * weight[i] * 0.1)
                T_new = T_delta @ T_curr
                poses[i] = matrix_to_quaternion(T_new)
        
        return poses

# ============================================================================
# MOTION MASK GENERATOR
# ============================================================================

class MotionMaskGenerator:
    def __init__(self, threshold=0.6):
        self.threshold = threshold
    
    def compute_mask(self, depth_i, depth_j, pose_i, pose_j, K):
        H, W = depth_i.shape
        device = depth_i.device
        
        T_i = quaternion_to_matrix(pose_i)
        T_j = quaternion_to_matrix(pose_j)
        T_relative = T_j @ torch.inverse(T_i)
        
        y_coords, x_coords = torch.meshgrid(
            torch.arange(H, device=device, dtype=torch.float32),
            torch.arange(W, device=device, dtype=torch.float32),
            indexing='ij'
        )
        ones = torch.ones_like(x_coords)
        pixel_coords = torch.stack([x_coords, y_coords, ones], dim=-1)
        
        K_inv = torch.inverse(K)
        points_3d = (K_inv @ pixel_coords.reshape(-1, 3).T).T.reshape(H, W, 3)
        points_3d = points_3d * depth_i.unsqueeze(-1)
        
        points_homo = torch.cat([points_3d, ones.unsqueeze(-1)], dim=-1)
        points_j = (T_relative @ points_homo.reshape(-1, 4).T).T.reshape(H, W, 4)[:, :, :3]
        
        pixels_j = (K @ points_j.reshape(-1, 3).T).T.reshape(H, W, 3)
        pixels_j_2d = pixels_j[:, :, :2] / (pixels_j[:, :, 2:] + 1e-8)
        
        grid = pixels_j_2d.clone()
        grid[:, :, 0] = 2.0 * grid[:, :, 0] / (W - 1) - 1.0
        grid[:, :, 1] = 2.0 * grid[:, :, 1] / (H - 1) - 1.0
        
        sampled_depth = F.grid_sample(
            depth_j.unsqueeze(0).unsqueeze(0),
            grid.unsqueeze(0),
            mode='bilinear',
            padding_mode='zeros',
            align_corners=True
        ).squeeze()
        
        depth_diff = torch.abs(sampled_depth - depth_i)
        
        valid_projection = (
            (pixels_j_2d[:, :, 0] >= 0) & 
            (pixels_j_2d[:, :, 0] < W) &
            (pixels_j_2d[:, :, 1] >= 0) & 
            (pixels_j_2d[:, :, 1] < H)
        )
        
        valid_depth = (depth_i > 0) & (sampled_depth > 0)
        small_depth_diff = depth_diff < self.threshold
        
        mask = valid_projection & valid_depth & small_depth_diff
        return mask

# ============================================================================
# STAGE 1 TRACKER
# ============================================================================

class Stage1Tracker:
    def __init__(self, checkpoint_path, device='cuda'):
        self.device = device
        print("\nInitializing Stage 1 Tracker...")
        
        self.model = DROIDNetwork().to(device)
        
        print(f"Loading checkpoint: {checkpoint_path}")
        checkpoint = torch.load(checkpoint_path, map_location=device)
        
        if 'model' in checkpoint:
            state_dict = checkpoint['model']
        elif 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        else:
            state_dict = checkpoint
        
        model_dict = self.model.state_dict()
        loaded_keys = 0
        
        for key, value in state_dict.items():
            clean_key = key.replace('module.', '').replace('net.', '')
            
            if clean_key in model_dict and value.shape == model_dict[clean_key].shape:
                model_dict[clean_key] = value
                loaded_keys += 1
        
        self.model.load_state_dict(model_dict, strict=False)
        self.model.eval()
        
        print(f"Loaded {loaded_keys}/{len(model_dict)} parameters")
        
        self.mask_generator = MotionMaskGenerator(threshold=0.6)
        print("Stage 1 Tracker ready!\n")
    
    def run(self, images, depths, intrinsics):
        N = images.shape[0]
        H, W = depths.shape[1], depths.shape[2]
        
        print(f"Processing {N} frames ({H}x{W})")
        
        print("\nStep 1: DROID-SLAM...")
        with torch.no_grad():
            poses_quat = self.model(images, depths, num_iterations=8)
        
        print(f"  -> Got {N} poses")
        
        print("\nStep 2: Motion masks...")
        masks = []
        
        for i in tqdm(range(N)):
            if i == 0:
                mask = torch.ones((H, W), dtype=torch.bool, device=self.device)
            else:
                mask = self.mask_generator.compute_mask(
                    depths[i-1], depths[i],
                    poses_quat[i-1], poses_quat[i],
                    intrinsics
                )
            masks.append(mask)
        
        masks = torch.stack(masks)
        print(f"  -> {masks.float().mean()*100:.1f}% static")
        
        poses_matrix = quaternion_to_matrix(poses_quat)
        
        return {
            'poses': poses_matrix,
            'motion_masks': masks,
            'num_frames': N
        }

# ============================================================================
# TUM DATASET LOADER
# ============================================================================

class TUMDataset:
    def __init__(self, dataset_path, sequence_name=None):
        self.dataset_path = Path(dataset_path)
        
        if sequence_name:
            self.sequence_path = self.dataset_path / sequence_name
        else:
            self.sequence_path = self.dataset_path
        
        if not self.sequence_path.exists():
            raise FileNotFoundError(f"Dataset not found at {self.sequence_path}")
        
        print(f"Loading TUM dataset: {self.sequence_path}")
        
        self.rgb_list = self._load_file_list('rgb.txt')
        self.depth_list = self._load_file_list('depth.txt')
        self.gt_poses = self._load_groundtruth('groundtruth.txt')
        self.associations = self._associate_frames()
        
        print(f"  Found {len(self.associations)} RGB-D pairs")
    
    def _load_file_list(self, filename):
        filepath = self.sequence_path / filename
        if not filepath.exists():
            return {}
        
        file_dict = {}
        with open(filepath, 'r') as f:
            for line in f:
                line = line.strip()
                if line.startswith('#') or len(line) == 0:
                    continue
                parts = line.split()
                timestamp = float(parts[0])
                filename = parts[1]
                file_dict[timestamp] = filename
        return file_dict
    
    def _load_groundtruth(self, filename):
        filepath = self.sequence_path / filename
        if not filepath.exists():
            return {}
        
        gt_dict = {}
        with open(filepath, 'r') as f:
            for line in f:
                line = line.strip()
                if line.startswith('#') or len(line) == 0:
                    continue
                parts = line.split()
                timestamp = float(parts[0])
                tx, ty, tz = float(parts[1]), float(parts[2]), float(parts[3])
                qx, qy, qz, qw = float(parts[4]), float(parts[5]), float(parts[6]), float(parts[7])
                pose = np.array([qw, qx, qy, qz, tx, ty, tz])
                gt_dict[timestamp] = pose
        return gt_dict
    
    def _associate_frames(self, max_time_diff=0.02):
        associations = []
        rgb_times = sorted(self.rgb_list.keys())
        depth_times = sorted(self.depth_list.keys())
        
        for rgb_time in rgb_times:
            closest_depth_time = min(depth_times, key=lambda t: abs(t - rgb_time))
            
            if abs(rgb_time - closest_depth_time) < max_time_diff:
                if self.gt_poses:
                    gt_times = list(self.gt_poses.keys())
                    closest_gt_time = min(gt_times, key=lambda t: abs(t - rgb_time))
                    
                    if abs(rgb_time - closest_gt_time) < max_time_diff:
                        associations.append({
                            'rgb_file': self.rgb_list[rgb_time],
                            'depth_file': self.depth_list[closest_depth_time],
                            'gt_pose': self.gt_poses[closest_gt_time]
                        })
        return associations
    
    def __len__(self):
        return len(self.associations)
    
    def load_sequence(self, start_idx=0, num_frames=None, stride=1):
        if num_frames is None:
            num_frames = len(self) - start_idx
        
        num_frames = min(num_frames, len(self) - start_idx)
        
        rgbs, depths, poses = [], [], []
        
        for i in range(start_idx, start_idx + num_frames * stride, stride):
            if i >= len(self):
                break
            
            assoc = self.associations[i]
            
            rgb_path = self.sequence_path / assoc['rgb_file']
            rgb = np.array(Image.open(rgb_path))
            rgbs.append(rgb)
            
            depth_path = self.sequence_path / assoc['depth_file']
            depth = np.array(Image.open(depth_path)).astype(np.float32) / 5000.0
            depths.append(depth)
            
            if 'gt_pose' in assoc:
                poses.append(assoc['gt_pose'])
        
        rgbs = np.stack(rgbs)
        depths = np.stack(depths)
        poses = np.stack(poses) if poses else None
        
        return rgbs, depths, poses
    
    @staticmethod
    def get_intrinsics(sequence_name, device='cuda'):
        if 'freiburg1' in sequence_name:
            fx, fy, cx, cy = 517.3, 516.5, 318.6, 255.3
        elif 'freiburg2' in sequence_name:
            fx, fy, cx, cy = 520.9, 521.0, 325.1, 249.7
        elif 'freiburg3' in sequence_name:
            fx, fy, cx, cy = 535.4, 539.2, 320.1, 247.6
        else:
            fx, fy, cx, cy = 535.4, 539.2, 320.1, 247.6
        
        K = torch.tensor([[fx, 0, cx], [0, fy, cy], [0, 0, 1]], device=device).float()
        return K

def download_tum_sequence(sequence_name, output_dir='./data/TUM'):
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    sequence_path = output_path / sequence_name
    
    if sequence_path.exists():
        print(f"Sequence exists at {sequence_path}")
        return str(sequence_path)
    
    print(f"Downloading {sequence_name}...")
    
    base_url = "https://vision.in.tum.de/rgbd/dataset/freiburg3"
    if 'freiburg2' in sequence_name:
        base_url = "https://vision.in.tum.de/rgbd/dataset/freiburg2"
    elif 'freiburg1' in sequence_name:
        base_url = "https://vision.in.tum.de/rgbd/dataset/freiburg1"
    
    url = f"{base_url}/{sequence_name}.tgz"
    tgz_file = output_path / f"{sequence_name}.tgz"
    
    subprocess.run(['wget', url, '-O', str(tgz_file)], check=True)
    print("Extracting...")
    subprocess.run(['tar', '-xzf', str(tgz_file), '-C', str(output_path)], check=True)
    tgz_file.unlink()
    
    print(f"Downloaded to {sequence_path}")
    return str(sequence_path)

# ============================================================================
# EVALUATION
# ============================================================================

def compute_ate(poses_est, poses_gt):
    if poses_est.shape[-1] == 7:
        T_est = quaternion_to_matrix(poses_est)
    else:
        T_est = poses_est
    
    if poses_gt.shape[-1] == 7:
        T_gt = quaternion_to_matrix(poses_gt)
    else:
        T_gt = poses_gt
    
    trans_est = T_est[:, :3, 3]
    trans_gt = T_gt[:, :3, 3]
    
    errors = torch.norm(trans_est - trans_gt, dim=1)
    ate = torch.sqrt(torch.mean(errors ** 2)).item()
    
    return ate

def plot_results(results, gt_poses=None):
    from mpl_toolkits.mplot3d import Axes3D
    
    poses = results['poses']
    masks = results['motion_masks']
    N = results['num_frames']
    
    trajectory = poses[:, :3, 3].cpu().numpy()
    
    fig = plt.figure(figsize=(15, 5))
    
    ax1 = fig.add_subplot(131, projection='3d')
    ax1.plot(trajectory[:, 0], trajectory[:, 1], trajectory[:, 2], 'b-', linewidth=2)
    ax1.scatter(trajectory[:, 0], trajectory[:, 1], trajectory[:, 2],
               c=range(N), cmap='viridis', s=30)
    
    if gt_poses is not None:
        if gt_poses.shape[-1] == 7:
            gt_T = quaternion_to_matrix(gt_poses)
        else:
            gt_T = gt_poses
        gt_traj = gt_T[:, :3, 3].cpu().numpy()
        ax1.plot(gt_traj[:, 0], gt_traj[:, 1], gt_traj[:, 2], 'r--', linewidth=2)
        
        ate = compute_ate(poses, gt_poses)
        ax1.set_title(f'Trajectory (ATE: {ate:.4f}m)')
    else:
        ax1.set_title('Trajectory')
    
    ax1.set_xlabel('X (m)')
    ax1.set_ylabel('Y (m)')
    ax1.set_zlabel('Z (m)')
    
    ax2 = fig.add_subplot(132)
    static_ratio = masks.float().mean(dim=(1,2)).cpu().numpy() * 100
    ax2.plot(range(N), static_ratio, 'b-', marker='o')
    ax2.set_xlabel('Frame')
    ax2.set_ylabel('Static (%)')
    ax2.set_title('Motion Masks')
    ax2.grid(True)
    
    ax3 = fig.add_subplot(133)
    mid_idx = N // 2
    mask_img = masks[mid_idx].cpu().numpy()
    ax3.imshow(mask_img, cmap='RdYlGn')
    ax3.set_title(f'Mask (Frame {mid_idx})')
    ax3.axis('off')
    
    plt.tight_layout()
    plt.show()

# ============================================================================
# MAIN
# ============================================================================

def main(sequence_name='rgbd_dataset_freiburg3_walking_xyz', 
         num_frames=100, 
         start_frame=0,
         download=True):
    
    print("="*70)
    print("DG-SLAM Stage 1 - TUM Evaluation")
    print("="*70)
    print(f"\nSequence: {sequence_name}")
    print(f"Frames: {num_frames} (start: {start_frame})")
    
    # Check/download dataset
    data_dir = './data/TUM'
    sequence_path = Path(data_dir) / sequence_name
    
    if not sequence_path.exists():
        if download:
            download_tum_sequence(sequence_name, data_dir)
        else:
            print(f"Error: Sequence not found at {sequence_path}")
            return
    
    # Load dataset
    dataset = TUMDataset(data_dir, sequence_name)
    
    # Load frames
    print(f"\nLoading {num_frames} frames...")
    rgbs, depths, gt_poses = dataset.load_sequence(start_frame, num_frames)
    
    print(f"RGB: {rgbs.shape}")
    print(f"Depth: {depths.shape}")
    print(f"GT: {gt_poses.shape if gt_poses is not None else 'None'}")
    
    # Convert to torch
    images = torch.from_numpy(rgbs).float().permute(0, 3, 1, 2) / 255.0
    images = images.to(device)
    depths_torch = torch.from_numpy(depths).float().to(device)
    
    if gt_poses is not None:
        poses_torch = torch.from_numpy(gt_poses).float().to(device)
    else:
        poses_torch = None
    
    K = TUMDataset.get_intrinsics(sequence_name, device)
    
    # Run Stage 1
    tracker = Stage1Tracker(CHECKPOINT_PATH, device=device)
    results = tracker.run(images, depths_torch, K)
    
    # Evaluate
    print("\n" + "="*70)
    print("RESULTS")
    print("="*70)
    
    if poses_torch is not None:
        ate = compute_ate(results['poses'], poses_torch)
        print(f"ATE: {ate:.6f} m ({ate*100:.2f} cm)")
    
    print(f"Frames: {results['num_frames']}")
    print(f"Static: {results['motion_masks'].float().mean()*100:.1f}%")
    
    # Plot
    plot_results(results, poses_torch)
    
    print("\nDone!")
    return results

# Run
if __name__ == "__main__":
    # Run on TUM walking sequence
    results = main(
        sequence_name='rgbd_dataset_freiburg3_walking_xyz',
        num_frames=100,
        start_frame=0,
        download=True
    )
